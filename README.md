# DD Analytics Agent - Technical Challenge ğŸš€

This is a technical assessment for candidates applying for Senior/Staff AI Engineer roles. It involves completing the implementation of a production-grade analytics agent skeleton.

## ğŸ¯ The Challenge

The objective is to "bring to life" a Planning Analytics Agent. While we have provided the deterministic foundation (engine, validation, contracts), the **agentic orchestration** and **tool logic** are stubbed out for you to implement.

**Please see [TASK.md](./TASK.md) for full instructions and success criteria.**

### Key Principles

1. **LLM does planning + specification**, not computation
2. **All outputs are typed contracts** validated before execution
3. **Execution is deterministic** - same specs always produce same results
4. **Full traceability** - target: each run saves artifacts (candidates wire this through the pipeline)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Azure OpenAI access (or mock for testing)

### Installation

```bash
# Clone and navigate to the project
cd dd-task

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e ".[dev]"

# Copy environment template
cp .env.example .env
# Edit .env with your Azure OpenAI credentials
```

### Configuration

Create a `.env` file with your Azure OpenAI credentials:

```bash
AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com
AZURE_OPENAI_API_KEY=your-api-key-here
AZURE_OPENAI_DEPLOYMENT=YOUR_MODEL_DEPLOYMENT_NAME
AZURE_OPENAI_API_VERSION=2024-08-01-preview
```

### Running the Demo

```bash
# Run the demo with the sample dataset
dd-agent demo

# Or run a specific analysis
dd-agent run --data data/demo --prompt "Show NPS by region"

# Run the full autoplan flow
dd-agent autoplan --data data/demo

# Validate your data directory
dd-agent validate --data data/demo
```

## ğŸ“ Project Structure

```
dd-agent/
â”œâ”€â”€ src/dd_agent/
â”‚   â”œâ”€â”€ cli.py              # Command-line interface
â”‚   â”œâ”€â”€ config.py           # Configuration (Azure OpenAI settings)
â”‚   â”œâ”€â”€ run_store.py        # Run artifact storage
â”‚   â”‚
â”‚   â”œâ”€â”€ contracts/          # Pydantic models (typed contracts)
â”‚   â”‚   â”œâ”€â”€ questions.py    # Question, Option, QuestionType
â”‚   â”‚   â”œâ”€â”€ filters.py      # Boolean expression AST
â”‚   â”‚   â”œâ”€â”€ specs.py        # CutSpec, SegmentSpec, MetricSpec
â”‚   â”‚   â”œâ”€â”€ tool_output.py  # ToolOutput envelope
â”‚   â”‚   â””â”€â”€ validate.py     # Domain validators (strict)
â”‚   â”‚
â”‚   â”œâ”€â”€ engine/             # Deterministic execution
â”‚   â”‚   â”œâ”€â”€ masks.py        # Filter â†’ boolean mask
â”‚   â”‚   â”œâ”€â”€ metrics.py      # Metric computations
â”‚   â”‚   â”œâ”€â”€ tables.py       # Table result models
â”‚   â”‚   â””â”€â”€ executor.py     # Main executor
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                # LLM integration
â”‚   â”‚   â”œâ”€â”€ azure_client.py # AzureOpenAI wrapper
â”‚   â”‚   â”œâ”€â”€ structured.py   # Structured outputs helpers
â”‚   â”‚   â””â”€â”€ prompts/        # Prompt templates
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/              # LLM-powered tools
â”‚   â”‚   â”œâ”€â”€ base.py         # Tool base class
â”‚   â”‚   â”œâ”€â”€ high_level_planner.py
â”‚   â”‚   â”œâ”€â”€ cut_planner.py
â”‚   â”‚   â””â”€â”€ segment_builder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrator/       # Pipeline coordination
â”‚   â”‚   â”œâ”€â”€ agent.py        # Agent class
â”‚   â”‚   â””â”€â”€ pipeline.py     # Pipeline flows
â”‚   â”‚
â”‚   â””â”€â”€ util/               # Utilities
â”‚       â”œâ”€â”€ hashing.py      # Dataset hashing
â”‚       â”œâ”€â”€ logging.py      # Structured logging
â”‚       â””â”€â”€ jsonschema.py   # JSON schema helpers
â”‚
â”œâ”€â”€ data/demo/              # Demo dataset
â”‚   â”œâ”€â”€ questions.json
â”‚   â”œâ”€â”€ responses.csv
â”‚   â”œâ”€â”€ scope.md
â”‚   â””â”€â”€ eval_cases.yaml
â”‚
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â”œâ”€â”€ test_validation.py
â”‚   â””â”€â”€ test_end_to_end_mock_llm.py
â”‚
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”§ Architecture

### Design Patterns

**Ports & Adapters (Hexagonal Architecture)**

- Core domain logic is isolated from the LLM provider
- LLM backend is an adapter: can swap AzureOpenAI â†” mock without touching core logic

**Typed Contracts + JSON Schema**

- All domain objects are Pydantic models
- LLM outputs use Structured Outputs + Pydantic validation for schema-conformant outputs (strict schema enforcement may be limited by provider features)

**Tool Interface with Standard Envelope**
Every tool returns:

```python
ToolOutput[T]:
    ok: bool
    data: T | None
    errors: list[ToolMessage]
    warnings: list[ToolMessage]
    trace: dict  # prompts, model, latency, mappings
```

### Data Flow

```
Input Files                 LLM Tools                 Executor              Output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€
questions.json  â”€â”€â”
responses.csv   â”€â”€â”¼â”€â”€â–¶  High-Level Planner â”€â”€â–¶ Cut Planner â”€â”€â–¶ Executor â”€â”€â–¶ Tables
scope.md        â”€â”€â”˜          â”‚                      â”‚              â”‚        Report
                             â–¼                      â–¼              â–¼
                      AnalysisIntents â”€â”€â”€â”€â–¶ CutSpecs (validated) â”€â”€â–¶ Results
```

## ğŸ“Š Contracts

### Question Types

- `single_choice` - Single selection from options
- `multi_choice` - Multiple selections (semicolon-separated)
- `likert_1_5` - 5-point Likert scale
- `likert_1_7` - 7-point Likert scale
- `numeric` - Numeric values
- `nps_0_10` - Net Promoter Score (0-10)

### Metric Types

| Metric | Compatible Types |
|--------|------------------|
| `frequency` | single_choice, multi_choice, likert, nps |
| `mean` | likert, numeric, nps |
| `top2box` | likert_1_5, likert_1_7 |
| `bottom2box` | likert_1_5, likert_1_7 |
| `nps` | nps_0_10 only |

### Filter Expression AST

```python
# Predicates
PredicateEq(question_id="Q1", value="option_a")
PredicateIn(question_id="Q1", values=["a", "b", "c"])
PredicateRange(question_id="Q1", min=18, max=65)
PredicateContainsAny(question_id="Q1", values=["x", "y"])  # multi-choice

# Logical Operators
And(children=[...])
Or(children=[...])
Not(child=...)
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test files
pytest tests/test_metrics.py -v
pytest tests/test_validation.py -v
pytest tests/test_end_to_end_mock_llm.py -v

# Run with coverage
pytest tests/ --cov=dd_agent --cov-report=html
```

### Test Categories

1. **Unit tests (no LLM)** - Metrics and validation
2. **End-to-end tests (mock LLM)** - Full pipeline with deterministic mocks
3. **Evaluation cases** - `eval_cases.yaml` for regression testing

## ğŸ“¦ Run Artifacts

Target run directory structure (once wired into the pipeline):

```
runs/<timestamp>_<id>/
â”œâ”€â”€ inputs/
â”‚   â”œâ”€â”€ questions.json
â”‚   â”œâ”€â”€ responses.csv
â”‚   â””â”€â”€ user_prompt.txt
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ high_level_plan.json
â”‚   â”œâ”€â”€ cuts.json
â”‚   â”œâ”€â”€ results.json
â”‚   â””â”€â”€ table_*.json
â”œâ”€â”€ metadata.json
â””â”€â”€ report.md
```

## ğŸš§ Production Hardening Notes

This skeleton is designed for local development. For production:

- **Authentication**: Use Entra ID instead of API keys
- **Caching**: Add prompt/result caching by hash
- **Observability**: Structured logs, per-tool latency, error rates
- **Safety**: PII scanning + redaction before LLM calls
- **Evals**: Expand `eval/` into regression tests

## ğŸ“ License

MIT License
